{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize a GPT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoModelForCausalLM, GPT2Tokenizer\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "vocab_size = 13\n",
    "sequence_length = 4\n",
    "result_length = 2\n",
    "context_length = sequence_length + result_length\n",
    "config = AutoConfig.from_pretrained(\"gpt2\", vocab_size=vocab_size, n_ctx=context_length, n_head=4, n_layer=2)\n",
    "model = AutoModelForCausalLM.from_config(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model will have the same architecture as GPT 2 but with a few modifications for making it smaller. The main changes are the size of the vocabulary that it is 13 because it will only handle numbers plus the padding token, the \"+\", and \"=\". The context window will only support 6 tokens since we are only interested in performing the addition of two single digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 15.0M parameters\n"
     ]
    }
   ],
   "source": [
    "def model_size(model):\n",
    "    return sum(t.numel() for t in model.parameters())\n",
    "\n",
    "print(f'Model size: {model_size(model)/1000**2:.1f}M parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model have 15 million weights instead of the 111 million parameters of the \"gpt2\" default config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ckpt = 'addition_model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pytorch_model.bin: 100%|██████████| 59.9M/59.9M [00:23<00:00, 2.59MB/s]\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained(\"models/\" + model_ckpt, push_to_hub=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom tokenizer to encode numbers and the padding token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumberTokenizer:\n",
    "  def __init__(self, numbers_qty=10):\n",
    "    vocab = ['+', '=', '-1', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "    self.numbers_qty = numbers_qty\n",
    "    self.pad_token = '-1'\n",
    "    self.encoder = {str(v):i for i,v in enumerate(vocab)}\n",
    "    self.decoder = {i:str(v) for i,v in enumerate(vocab)}\n",
    "    self.pad_token_id = self.encoder[self.pad_token]\n",
    "\n",
    "  def decode(self, token_ids):\n",
    "    return ' '.join(self.decoder[t] for t in token_ids)\n",
    "\n",
    "  def __call__(self, text):\n",
    "    return [self.encoder[t] for t in text.split()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of the tokenization for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 0, 4, 1, 5]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = NumberTokenizer(vocab_size)\n",
    "tokenizer(\"1 + 1 = 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '+',\n",
       " 1: '=',\n",
       " 2: '-1',\n",
       " 3: '0',\n",
       " 4: '1',\n",
       " 5: '2',\n",
       " 6: '3',\n",
       " 7: '4',\n",
       " 8: '5',\n",
       " 9: '6',\n",
       " 10: '7',\n",
       " 11: '8',\n",
       " 12: '9'}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Built dataset with unsorted and sorted sequences of numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class AdditionDataset(Dataset):\n",
    "    \"\"\" \n",
    "    Dataset with example of the form:\n",
    "    Input: \"2 + 3 = 0\" where the first 4 characters represent the initial input sequence\n",
    "    Output: \"+ 3 = 0 5\" where the last ditis represents the result of the addition and the first 3 digits are ignored during training\n",
    "    The result is a dataset of tokenized sequences of numbers\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, split, length=6):\n",
    "        assert split in {'train', 'test'}\n",
    "        self.split = split\n",
    "        self.length = length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return 1000000 # 1M examples per split\n",
    "    \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # The dataset is generated on the fly\n",
    "\n",
    "        available_numbers = [int(n) for n in tokenizer.decoder.values() if n != tokenizer.pad_token and str(n).isnumeric()]\n",
    "        # generate some random integers\n",
    "        inp = torch.tensor(np.random.choice(available_numbers, size=result_length))\n",
    "        # solve the task\n",
    "        sol = torch.tensor([int(i) for i in str(inp.sum().item())])\n",
    "        sol = torch.nn.functional.pad(sol, (1 if sol.size()[0] == 1 else 0,0), 'constant', 0)\n",
    "\n",
    "        \n",
    "        # concatenate the problem specification and the solution\n",
    "        cat = torch.cat((inp, sol), dim=0)\n",
    "\n",
    "        # the inputs to the transformer will be the offset sequence\n",
    "        x = cat[:-1].clone()\n",
    "        y = cat[1:].clone()\n",
    "        # we only want to predict at output locations, mask out the loss at the input locations\n",
    "        y[:1] = int(tokenizer.pad_token)\n",
    "\n",
    "        # Convert the tensors to the input expected by the model\n",
    "        x = str(x[0].item()) + ' + ' + str(x[1].item()) + ' = ' + str(x[2].item())\n",
    "        y = '-1 ' + str(y[0].item()) + ' -1 ' + str(y[1].item()) + ' ' + str(y[2].item())\n",
    "        # tokenize the input and targets\n",
    "        tokenized_input = tokenizer(x)\n",
    "        tokenized_output = tokenizer(y)\n",
    "        return torch.tensor(tokenized_input), torch.tensor(tokenized_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the training and testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = AdditionDataset('train', length=sequence_length)\n",
    "test_dataset = AdditionDataset('test', length=sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 + 7 = 1\n",
      "-1 -1 -1 1 2\n"
     ]
    }
   ],
   "source": [
    "x, y = train_dataset[0]\n",
    "print(tokenizer.decode(x.numpy()))\n",
    "print(tokenizer.decode(y.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_solution(input, solution_length=6, model=model):\n",
    "  model.eval()\n",
    "  input = torch.tensor(tokenizer(input))\n",
    "  input = input.to(accelerator.device)\n",
    "  solution = []\n",
    "  for i in range(solution_length):\n",
    "    output = model(input)\n",
    "    predicted = output.logits[-1].argmax()\n",
    "    input = torch.cat((input, predicted.unsqueeze(0)), dim=0)\n",
    "    solution.append(predicted.cpu().item())\n",
    "  return tokenizer.decode(solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/1 loss: 0.18201486766338348\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "\n",
    "batch_size = 100\n",
    "num_epochs = 1\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "dataset = train_dataset\n",
    "data = torch.utils.data.DataLoader(dataset, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "model, optimizer, data = accelerator.prepare(model, optimizer, data)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "  for source, targets in data:\n",
    "    optimizer.zero_grad()\n",
    "    loss = F.cross_entropy(model(source).logits.flatten(end_dim=1), targets.flatten(end_dim=1), ignore_index=tokenizer.pad_token_id)\n",
    "    accelerator.backward(loss)\n",
    "    optimizer.step()\n",
    "    loss = F.cross_entropy(model(source).logits.flatten(end_dim=1), targets.flatten(end_dim=1), ignore_index=tokenizer.pad_token_id)\n",
    "  print(f'Epoch: {epoch+1}/{num_epochs} loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(num_samples=1000, log=False):\n",
    "  correct = 0\n",
    "  for i in range(num_samples):\n",
    "    input, target = test_dataset[i]\n",
    "    input = input.cpu().numpy()\n",
    "    target = target.cpu().numpy()\n",
    "    input = tokenizer.decode(input[:sequence_length])\n",
    "    target = tokenizer.decode(target[sequence_length-1:])\n",
    "    predicted = generate_solution(input, solution_length=result_length, model=model)\n",
    "    if target == predicted:\n",
    "      correct += 1\n",
    "      if log:\n",
    "        print(f'CORRECT  Input: {input} Target: {target} Predicted: {predicted}')\n",
    "    else:\n",
    "      if log:\n",
    "        print(f'Input: {input} Target: {target} Predicted: {predicted}')\n",
    "\n",
    "  print(f'Accuracy: {correct/num_samples}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.946\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(num_samples=1000, log=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORRECT  Input: 6 + 3 = Target: 0 9 Predicted: 0 9\n",
      "CORRECT  Input: 0 + 4 = Target: 0 4 Predicted: 0 4\n",
      "CORRECT  Input: 4 + 4 = Target: 0 8 Predicted: 0 8\n",
      "CORRECT  Input: 0 + 4 = Target: 0 4 Predicted: 0 4\n",
      "CORRECT  Input: 7 + 6 = Target: 1 3 Predicted: 1 3\n",
      "CORRECT  Input: 4 + 7 = Target: 1 1 Predicted: 1 1\n",
      "CORRECT  Input: 9 + 8 = Target: 1 7 Predicted: 1 7\n",
      "CORRECT  Input: 5 + 5 = Target: 1 0 Predicted: 1 0\n",
      "CORRECT  Input: 1 + 3 = Target: 0 4 Predicted: 0 4\n",
      "CORRECT  Input: 9 + 9 = Target: 1 8 Predicted: 1 8\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(num_samples=10, log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pytorch_model.bin: 100%|██████████| 59.9M/59.9M [00:37<00:00, 1.60MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Manuel2011/addition_model/commit/d3600b85f53ef59f02dd579bc15326a618d62c39', commit_message='Upload model', commit_description='', oid='d3600b85f53ef59f02dd579bc15326a618d62c39', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub(\"models/\" + model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
