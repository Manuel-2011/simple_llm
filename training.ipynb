{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize a GPT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/manuel.mosquera/Documents/projects/simpleLLM/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig, AutoModelForCausalLM\n",
    "\n",
    "vocab_size = 13\n",
    "sequence_length = 4\n",
    "result_length = 2\n",
    "context_length = sequence_length + result_length\n",
    "config = AutoConfig.from_pretrained(\"gpt2\", vocab_size=vocab_size, n_ctx=context_length, n_head=12, n_layer=6, n_positions=context_length, n_embd=192)\n",
    "model = AutoModelForCausalLM.from_config(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model will have the same architecture as GPT 2 but with a few modifications for making it smaller. The main changes are the size of the vocabulary that it is 13 because it will only handle numbers plus the padding token, the \"+\", and \"=\". The context window will only support 6 tokens since we are only interested in performing the addition of two single digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 2.7M parameters\n"
     ]
    }
   ],
   "source": [
    "def model_size(model):\n",
    "    return sum(t.numel() for t in model.parameters())\n",
    "\n",
    "print(f'Model size: {model_size(model)/1000**2:.1f}M parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model has 2.7 million weights instead of the 124 million parameters of the \"gpt2\" default config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ckpt = 'addition_model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pytorch_model.bin: 100%|██████████| 10.7M/10.7M [00:15<00:00, 684kB/s] \n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained(\"models/\" + model_ckpt, push_to_hub=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom tokenizer to encode numbers and the padding token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumberTokenizer:\n",
    "  def __init__(self, numbers_qty=10):\n",
    "    vocab = ['+', '=', '-1', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "    self.numbers_qty = numbers_qty\n",
    "    self.pad_token = '-1'\n",
    "    self.encoder = {str(v):i for i,v in enumerate(vocab)}\n",
    "    self.decoder = {i:str(v) for i,v in enumerate(vocab)}\n",
    "    self.pad_token_id = self.encoder[self.pad_token]\n",
    "\n",
    "  def decode(self, token_ids):\n",
    "    return ' '.join(self.decoder[t] for t in token_ids)\n",
    "\n",
    "  def __call__(self, text):\n",
    "    return [self.encoder[t] for t in text.split()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of the tokenization for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 0, 4, 1, 5]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = NumberTokenizer(vocab_size)\n",
    "tokenizer(\"1 + 1 = 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '+',\n",
       " 1: '=',\n",
       " 2: '-1',\n",
       " 3: '0',\n",
       " 4: '1',\n",
       " 5: '2',\n",
       " 6: '3',\n",
       " 7: '4',\n",
       " 8: '5',\n",
       " 9: '6',\n",
       " 10: '7',\n",
       " 11: '8',\n",
       " 12: '9'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Built the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class AdditionDataset(Dataset):\n",
    "    \"\"\" \n",
    "    Dataset with example of the form:\n",
    "    Input: \"2 + 3 = 0\" where the first 4 characters represent the initial input sequence\n",
    "    Output: \"+ 3 = 0 5\" where the last ditis represents the result of the addition and the first 3 digits are ignored during training\n",
    "    The result is a dataset of tokenized sequences of numbers\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, split, length=6):\n",
    "        assert split in {'train', 'test'}\n",
    "        self.split = split\n",
    "        self.length = length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return 10000 # 10.000 examples per split\n",
    "    \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # The dataset is generated on the fly\n",
    "\n",
    "        available_numbers = [int(n) for n in tokenizer.decoder.values() if n != tokenizer.pad_token and str(n).isnumeric()]\n",
    "        # generate some random integers\n",
    "        inp = torch.tensor(np.random.choice(available_numbers, size=result_length))\n",
    "        # solve the task\n",
    "        sol = torch.tensor([int(i) for i in str(inp.sum().item())])\n",
    "        sol = torch.nn.functional.pad(sol, (1 if sol.size()[0] == 1 else 0,0), 'constant', 0)\n",
    "\n",
    "        \n",
    "        # concatenate the problem specification and the solution\n",
    "        cat = torch.cat((inp, sol), dim=0)\n",
    "\n",
    "        # the inputs to the transformer will be the offset sequence\n",
    "        x = cat[:-1].clone()\n",
    "        y = cat[1:].clone()\n",
    "        # we only want to predict at output locations, mask out the loss at the input locations\n",
    "        y[:1] = int(tokenizer.pad_token)\n",
    "\n",
    "        # Convert the tensors to the input expected by the model\n",
    "        x = str(x[0].item()) + ' + ' + str(x[1].item()) + ' = ' + str(x[2].item())\n",
    "        y = '-1 ' + str(y[0].item()) + ' -1 ' + str(y[1].item()) + ' ' + str(y[2].item())\n",
    "        # tokenize the input and targets\n",
    "        tokenized_input = tokenizer(x)\n",
    "        tokenized_output = tokenizer(y)\n",
    "        return torch.tensor(tokenized_input), torch.tensor(tokenized_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the training and testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = AdditionDataset('train', length=sequence_length)\n",
    "test_dataset = AdditionDataset('test', length=sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 + 2 = 0\n",
      "-1 -1 -1 0 3\n"
     ]
    }
   ],
   "source": [
    "x, y = train_dataset[0]\n",
    "print(tokenizer.decode(x.numpy()))\n",
    "print(tokenizer.decode(y.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100 loss: 0.26241305470466614\n",
      "Step: 200 loss: 0.31865572929382324\n",
      "Epoch: 1/1 loss: 0.07760939747095108\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "\n",
    "batch_size = 40\n",
    "num_epochs = 1\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "dataset = train_dataset\n",
    "data = torch.utils.data.DataLoader(dataset, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "model, optimizer, data = accelerator.prepare(model, optimizer, data)\n",
    "\n",
    "model.train()\n",
    "i = 0\n",
    "for epoch in range(num_epochs):\n",
    "  for source, targets in data:\n",
    "    i += 1\n",
    "    optimizer.zero_grad()\n",
    "    loss = F.cross_entropy(model(source).logits.flatten(end_dim=1), targets.flatten(end_dim=1), ignore_index=tokenizer.pad_token_id)\n",
    "    accelerator.backward(loss)\n",
    "    optimizer.step()\n",
    "    if i % 100 == 0:\n",
    "      print(f'Step: {i} loss: {loss.item()}')\n",
    "  print(f'Epoch: {epoch+1}/{num_epochs} loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_solution(input, solution_length=6, model=model):\n",
    "  model.eval()\n",
    "  input = torch.tensor(tokenizer(input))\n",
    "  input = input.to(accelerator.device)\n",
    "  solution = []\n",
    "  for i in range(solution_length):\n",
    "    output = model(input)\n",
    "    predicted = output.logits[-1].argmax()\n",
    "    input = torch.cat((input, predicted.unsqueeze(0)), dim=0)\n",
    "    solution.append(predicted.cpu().item())\n",
    "  return tokenizer.decode(solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(num_samples=1000, log=False):\n",
    "  correct = 0\n",
    "  for i in range(num_samples):\n",
    "    input, target = test_dataset[i]\n",
    "    input = input.cpu().numpy()\n",
    "    target = target.cpu().numpy()\n",
    "    input = tokenizer.decode(input[:sequence_length])\n",
    "    target = tokenizer.decode(target[sequence_length-1:])\n",
    "    predicted = generate_solution(input, solution_length=result_length, model=model)\n",
    "    if target == predicted:\n",
    "      correct += 1\n",
    "      if log:\n",
    "        print(f'CORRECT  Input: {input} Target: {target} Predicted: {predicted}')\n",
    "    else:\n",
    "      if log:\n",
    "        print(f'Input: {input} Target: {target} Predicted: {predicted}')\n",
    "\n",
    "  print(f'Accuracy: {correct/num_samples}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(num_samples=1000, log=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORRECT  Input: 6 + 9 = Target: 1 5 Predicted: 1 5\n",
      "CORRECT  Input: 1 + 1 = Target: 0 2 Predicted: 0 2\n",
      "CORRECT  Input: 9 + 6 = Target: 1 5 Predicted: 1 5\n",
      "CORRECT  Input: 8 + 9 = Target: 1 7 Predicted: 1 7\n",
      "CORRECT  Input: 1 + 8 = Target: 0 9 Predicted: 0 9\n",
      "CORRECT  Input: 4 + 4 = Target: 0 8 Predicted: 0 8\n",
      "CORRECT  Input: 0 + 2 = Target: 0 2 Predicted: 0 2\n",
      "CORRECT  Input: 0 + 8 = Target: 0 8 Predicted: 0 8\n",
      "CORRECT  Input: 2 + 6 = Target: 0 8 Predicted: 0 8\n",
      "CORRECT  Input: 6 + 5 = Target: 1 1 Predicted: 1 1\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(num_samples=10, log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pytorch_model.bin: 100%|██████████| 10.7M/10.7M [00:10<00:00, 1.05MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Manuel2011/addition_model/commit/5cfb2cfd1d72b612db88985d84be0f1427608b6a', commit_message='Upload model', commit_description='', oid='5cfb2cfd1d72b612db88985d84be0f1427608b6a', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub(\"models/\" + model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
