{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize a GPT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoModelForCausalLM, GPT2Tokenizer\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "vocab_size = 4\n",
    "sequence_length = 4\n",
    "context_length = sequence_length*2 - 1\n",
    "config = AutoConfig.from_pretrained(\"gpt2\", vocab_size=vocab_size, n_ctx=context_length, n_head=4, n_layer=2)\n",
    "model = AutoModelForCausalLM.from_config(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model will have the same architecture as GPT 2 but with a few modifications for making it smaller. The main changes are the size of the vocabulary that it is 4 because it will only handle numbers plus a token for padding, and the context window will only support 7 tokens since we are only interested in sorting numbers of a fix length of 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 15.0M parameters\n"
     ]
    }
   ],
   "source": [
    "def model_size(model):\n",
    "    return sum(t.numel() for t in model.parameters())\n",
    "\n",
    "print(f'Model size: {model_size(model)/1000**2:.1f}M parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model have 15 million weights instead of the 111 million parameters of the \"gpt2\" default config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ckpt = 'sortingLLM'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"models/\" + model_ckpt, push_to_hub=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom tokenizer to encode numbers and the padding token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumberTokenizer:\n",
    "  def __init__(self, numbers_qty=10):\n",
    "    self.numbers_qty = numbers_qty\n",
    "    self.pad_token = '-1'\n",
    "    self.encoder = {str(v):i for i,v in enumerate(range(-1, numbers_qty-1))}\n",
    "    self.decoder = {i:str(v) for i,v in enumerate(range(-1, numbers_qty-1))}\n",
    "    self.pad_token_id = self.encoder[self.pad_token]\n",
    "\n",
    "  def decode(self, token_ids):\n",
    "    return ' '.join(self.decoder[t] for t in token_ids)\n",
    "\n",
    "  def __call__(self, text):\n",
    "    return [self.encoder[t] for t in text.split()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of the tokenization for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 1, 2, 2, 3]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = NumberTokenizer(vocab_size)\n",
    "tokenizer(\"1 0 1 1 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '-1', 1: '0', 2: '1', 3: '2'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Built dataset with unsorted and sorted sequences of numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SortDataset(Dataset):\n",
    "    \"\"\" \n",
    "    Dataset with example of the form:\n",
    "    Input: \"0 3 1 0 2 1 0 0 1 1 2\" where the first 6 digits represent the initial input sequence\n",
    "    Output: \"3 1 0 2 1 0 0 1 1 2 3\" where the last ditis represents the sorted sequence and the 5 first digits are ignored during training\n",
    "    The result is a dataset of tokenized sequences of numbers\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, split, length=6):\n",
    "        assert split in {'train', 'test'}\n",
    "        self.split = split\n",
    "        self.length = length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return 1000000 # 1M examples per split\n",
    "    \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # The dataset is generated on the fly\n",
    "\n",
    "        available_numbers = [int(n) for n in tokenizer.decoder.values() if n != tokenizer.pad_token]\n",
    "        # For training we will generate easy examples, i.e. with a small number of possible digits\n",
    "        # if self.split == 'train':\n",
    "        #     available_numbers = available_numbers[:-2]\n",
    "        # generate some random integers\n",
    "        inp = torch.tensor(np.random.choice(available_numbers, size=self.length))\n",
    "        # solve the task\n",
    "        sol = torch.sort(inp)[0]\n",
    "\n",
    "        # concatenate the problem specification and the solution\n",
    "        cat = torch.cat((inp, sol), dim=0)\n",
    "\n",
    "        # the inputs to the transformer will be the offset sequence\n",
    "        x = cat[:-1].clone()\n",
    "        y = cat[1:].clone()\n",
    "        # we only want to predict at output locations, mask out the loss at the input locations\n",
    "        y[:self.length-1] = int(tokenizer.pad_token)\n",
    "\n",
    "        # Convert the tensors to the input expected by the model\n",
    "        x, y = ' '.join(map(str, x.tolist())), ' '.join(map(str, y.tolist()))\n",
    "        # tokenize the input and targets\n",
    "        tokenized_input = tokenizer(x)\n",
    "        tokenized_output = tokenizer(y)\n",
    "        return torch.tensor(tokenized_input), torch.tensor(tokenized_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the training and testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SortDataset('train', length=sequence_length)\n",
    "test_dataset = SortDataset('test', length=sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([3, 3, 3, 2, 2, 3, 3]), tensor([0, 0, 0, 2, 3, 3, 3]))\n",
      "(tensor([1, 3, 1, 2, 1, 1, 2]), tensor([0, 0, 0, 1, 1, 2, 3]))\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0])\n",
    "print(test_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_solution(input, solution_length=6, model=model):\n",
    "  model.eval()\n",
    "  input = torch.tensor(tokenizer(input))\n",
    "  input = input.to(accelerator.device)\n",
    "  solution = []\n",
    "  for i in range(solution_length):\n",
    "    output = model(input)\n",
    "    predicted = output.logits[-1].argmax()\n",
    "    input = torch.cat((input, predicted.unsqueeze(0)), dim=0)\n",
    "    solution.append(predicted.cpu().item())\n",
    "  return tokenizer.decode(solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2 loss: 0.05919044837355614\n",
      "Epoch: 2/2 loss: 0.0646464079618454\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "\n",
    "batch_size = 100\n",
    "num_epochs = 2\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "dataset = train_dataset\n",
    "data = torch.utils.data.DataLoader(dataset, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "model, optimizer, data = accelerator.prepare(model, optimizer, data)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "  for source, targets in data:\n",
    "    optimizer.zero_grad()\n",
    "    loss = F.cross_entropy(model(source).logits.flatten(end_dim=1), targets.flatten(end_dim=1), ignore_index=tokenizer.pad_token_id)\n",
    "    accelerator.backward(loss)\n",
    "    optimizer.step()\n",
    "    loss = F.cross_entropy(model(source).logits.flatten(end_dim=1), targets.flatten(end_dim=1), ignore_index=tokenizer.pad_token_id)\n",
    "  print(f'Epoch: {epoch+1}/{num_epochs} loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(num_samples=1000, log=False):\n",
    "  correct = 0\n",
    "  for i in range(num_samples):\n",
    "    input, target = test_dataset[i]\n",
    "    input = input.cpu().numpy()\n",
    "    target = target.cpu().numpy()\n",
    "    input = tokenizer.decode(input[:sequence_length])\n",
    "    target = tokenizer.decode(target[sequence_length-1:])\n",
    "    predicted = generate_solution(input, solution_length=sequence_length, model=model)\n",
    "    if target == predicted:\n",
    "      correct += 1\n",
    "      if log:\n",
    "        print(f'CORRECT  Input: {input} Target: {target} Predicted: {predicted}')\n",
    "    else:\n",
    "      if log:\n",
    "        print(f'Input: {input} Target: {target} Predicted: {predicted}')\n",
    "\n",
    "  print(f'Accuracy: {correct/num_samples}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(num_samples=1000, log=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORRECT  Input: 0 1 0 0 Target: 0 0 0 1 Predicted: 0 0 0 1\n",
      "CORRECT  Input: 0 0 1 0 Target: 0 0 0 1 Predicted: 0 0 0 1\n",
      "CORRECT  Input: 1 1 0 1 Target: 0 1 1 1 Predicted: 0 1 1 1\n",
      "CORRECT  Input: 1 2 0 0 Target: 0 0 1 2 Predicted: 0 0 1 2\n",
      "CORRECT  Input: 1 0 1 2 Target: 0 1 1 2 Predicted: 0 1 1 2\n",
      "CORRECT  Input: 1 0 1 0 Target: 0 0 1 1 Predicted: 0 0 1 1\n",
      "CORRECT  Input: 2 1 1 1 Target: 1 1 1 2 Predicted: 1 1 1 2\n",
      "CORRECT  Input: 1 1 0 1 Target: 0 1 1 1 Predicted: 0 1 1 1\n",
      "CORRECT  Input: 2 2 1 1 Target: 1 1 2 2 Predicted: 1 1 2 2\n",
      "CORRECT  Input: 2 0 2 2 Target: 0 2 2 2 Predicted: 0 2 2 2\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(num_samples=10, log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pytorch_model.bin: 100%|██████████| 59.9M/59.9M [00:25<00:00, 2.37MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Manuel2011/sortingLLM/commit/f495c49854785f6a8dac60f25dad5da1fd7fe77a', commit_message='Upload model', commit_description='', oid='f495c49854785f6a8dac60f25dad5da1fd7fe77a', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub(\"models/\" + model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
